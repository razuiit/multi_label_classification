import argparseimport osimport stringfrom collections import Counterfrom typing import List, Tuple, Dictimport numpy as npfrom tqdm import tqdmfrom transformers import BertTokenizerimport configfrom model.unsupervised.encoding import DocumentEncoderfrom util.file_io import save_static, save_tokenization, load_preprocessedfrom util.utils import model_from_pretrainedclass SentencePreparer:    def __init__(self,                 model_name: str):        self.tokenizer = model_from_pretrained(model_class=BertTokenizer, model_name=model_name)    def basic_tokenize(self,                       text: str) -> List[str]:        return self.tokenizer.basic_tokenizer.tokenize(text, never_split=self.tokenizer.all_special_tokens)    def prepare_document(self,                         text: str) -> Tuple[List[str], List[Tuple[int, int, int]], List[List[int]]]:        tokenized_text = self.tokenizer.basic_tokenizer.tokenize(text, never_split=self.tokenizer.all_special_tokens)        tokenized_to_id_indicies, tokenids_chunks, tokenids_chunk = [], [], []        for token in tokenized_text + [None]:            if token is not None:                wp_tokens = self.tokenizer.wordpiece_tokenizer.tokenize(token)            if (token is None) or (len(tokenids_chunk) + len(wp_tokens) > config.MAX_TOKENS):                tokenids_chunks.append([self.tokenizer.cls_token_id] + tokenids_chunk + [self.tokenizer.sep_token_id])                tokenids_chunk = tokenids_chunk[-config.SLIDING_WINDOW_SIZE:]            if token is not None:                tokenized_to_id_indicies.append((len(tokenids_chunks),                                                 len(tokenids_chunk),                                                 len(tokenids_chunk) + len(wp_tokens)))                tokenids_chunk.extend(self.tokenizer.convert_tokens_to_ids(wp_tokens))        return tokenized_text, tokenized_to_id_indicies, tokenids_chunksdef __static_representation(model_name: str,                            texts: List[str]) -> Tuple[    List[Tuple[List[str], List[Tuple[int, int, int]], List[List[int]]]],  # tokenization_info    List[np.ndarray],  # doc_words_reprs    np.ndarray,  # word_reprs    List[str],  # vocab_words    Dict[str, int]  # word_counts]:    preparer = SentencePreparer(model_name)    encoder = DocumentEncoder(model_name)    # count word frequencies    word_counts = Counter()    print('counting word frequencies ...')    for text in tqdm(texts):        tokenized_text = preparer.basic_tokenize(text)        word_counts.update(word.translate(str.maketrans('', '', string.punctuation)) for word in tokenized_text)    del word_counts['']    word_counts = dict(word_counts)    tokenization_info = []    doc_words_reprs = []    word_reprs = dict()    print('encoding words ...')    for text in tqdm(texts):        tokenized_text, tokenized_to_id_indicies, tokenids_chunks = preparer.prepare_document(text)        tokenization_info.append((tokenized_text, tokenized_to_id_indicies, tokenids_chunks))        # word embedding        doc_words_repr = encoder.encode_document(tokenized_text,                                                 tokenized_to_id_indicies,                                                 tokenids_chunks)        doc_words_reprs.append(doc_words_repr)        for word, word_embedding in zip(tokenized_text, doc_words_repr):            if word in word_counts.keys():                word_reprs[word] = word_reprs[word] + word_embedding if word in word_reprs else word_embedding    word_reprs = {k: v / word_counts[k] for k, v in word_reprs.items()}    vocab_words = list(word_reprs.keys())    word_reprs = np.array(list(word_reprs.values()))    return tokenization_info, doc_words_reprs, word_reprs, vocab_words, word_countsdef run(texts: List[str],        classes: List[str],        model_name: str) -> Tuple[    np.ndarray,  # word_reprs    List[str],  # vocab_words    Dict[str, int],  # word_counts    List[Tuple[List[str], List[Tuple[int, int, int]], List[List[int]]]],  # text_tokenization_info    List[Tuple[List[str], List[Tuple[int, int, int]], List[List[int]]]],  # class_tokenization_info    List[np.ndarray],  # text_words_reprs    List[np.ndarray],  # class_words_reprs]:    # texts = df[config.PREPROCESSED_TEXT_COL].to_list()    num_texts = len(texts)    texts += [clas.replace(config.EMPTY_TYPE, '').replace(config.JOIN_CHAR, ' ') for clas in classes]    tokenization_info, doc_words_reprs, word_rep, vocab_words, word_counts = __static_representation(model_name, texts)    text_tokenization_info = tokenization_info[:num_texts]    class_tokenization_info = tokenization_info[num_texts:]    text_words_reprs = doc_words_reprs[:num_texts]    class_words_reprs = doc_words_reprs[num_texts:]    word_to_index = {v: k for k, v in enumerate(vocab_words)}    vocab_occurrence = list(word_counts.values())    print(f"`{os.path.splitext(os.path.basename(__file__))[0]}` is done.")    return (word_rep, vocab_words, word_to_index, vocab_occurrence, text_tokenization_info, class_tokenization_info,            text_words_reprs, class_words_reprs)if __name__ == '__main__':    parser = argparse.ArgumentParser()    parser.add_argument("--data_dir", type=str, required=True)    parser.add_argument("--model_name", type=str, required=True)    args = parser.parse_args()    data = load_preprocessed(args.data_dir)    _, df, classes = data[0]    (        word_rep, vocab_words, word_to_index, vocab_occurrence, text_tokenization_info, class_tokenization_info,        text_words_reprs, class_words_reprs    ) = run(df, classes, args.model_name)    save_tokenization(args.data_dir, text_tokenization_info, class_tokenization_info)    save_static(args.data_dir, word_rep, vocab_words, word_to_index, vocab_occurrence, text_words_reprs,                class_words_reprs)